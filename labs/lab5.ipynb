{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNElQIgdBxNu"
      },
      "source": [
        "# Laboratorio 5: Introducción a Redes Neuronales Recurrentes.\n",
        "\n",
        "##### <strong>RNN y PyTorch </strong>\n",
        "\n",
        "### Cuerpo Docente\n",
        "\n",
        "- Profesores: [Andrés Abeliuk](https://aabeliuk.github.io/), [Fabián Villena](https://villena.cl/).\n",
        "- Profesor Auxiliar: Martín Paredes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchvision torchaudio\n",
        "\n",
        "!pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxBT66X5-r4t",
        "outputId": "53203f13-c5fc-406d-9ded-9bfea164af54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Would remove:\n",
            "    /usr/local/bin/torchfrtrace\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.12/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torch-2.8.0+cu126.dist-info/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision-0.23.0+cu126.dist-info/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libcudart.45e7f3ed.so.12\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libjpeg.bd6b9199.so.8\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libnvjpeg.e5f20359.so.12\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libpng16.0481ee11.so.16\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libsharpyuv.b609dd4c.so.0\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libwebp.58a855fe.so.7\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision.libs/libz.622bbd06.so.1\n",
            "    /usr/local/lib/python3.12/dist-packages/torchvision/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.12/dist-packages/torchaudio-2.8.0+cu126.dist-info/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torchaudio/*\n",
            "    /usr/local/lib/python3.12/dist-packages/torio/*\n",
            "Proceed (Y/n)? "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall scipy\n",
        "!pip install scipy==1.11.4"
      ],
      "metadata": {
        "id": "1dveO1RAAl5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6729h9WHDiFa"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wNXQdQeDjIt"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzcewsGzfibI"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "import torch\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "from torch.optim import Adam\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import scikitplot as skplt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bEyTzRTDjlD"
      },
      "source": [
        "## Repaso: Redes Neuronales Recurrentes para Clasificación de texto\n",
        "\n",
        "Los tipos de redes neuronales como las redes fully connected  o las convolucionales son buenas para identificar patrones en los datos, pero no tienen memoria. Tratan cada ejemplo de datos y partes del ejemplo como independientes entre sí. No pueden mantener ningún estado/memoria sobre ejemplos previamente vistos. Este tipo de comportamiento es bueno siempre y cuando cada ejemplo, como las imágenes, sea independiente de los demás. Pero hay situaciones en las que recordar información de estado sobre ejemplos previamente vistos puede ayudar a obtener mejores resultados. Digamos, por ejemplo, en la tarea de procesamiento de lenguaje natural de generación de texto, si nuestra red puede recordar alguna información de estado sobre las palabras vistas, entonces puede ayudar a recordar el estado y generar nuevas palabras mejores ya que ahora conoce el contexto de la oración. Este tipo de enfoque también puede ayudar con datos de series temporales, donde la nueva predicción generalmente depende de los últimos ejemplos de texto.\n",
        "\n",
        "Para resolver el problema de mantener la memoria, se introdujeron las redes neuronales recurrentes (RNN). Las redes neuronales recurrentes mantienen el estado de los ejemplos de datos y lo utilizan para mejorar los resultados. Si el lector está interesado en aprender sobre el funcionamiento interno de las RNN, recomendamos este blog que lo cubre en detalle.\n",
        "\n",
        "Como parte de este tutorial, vamos a diseñar RNNs simples usando PyTorch para resolver tareas de clasificación de texto. Probaremos diferentes enfoques para usar RNNs en la clasificación de documentos de texto. Utilizaremos el enfoque de incrustación de palabras para vectorizar palabras en vectores de valores reales antes de proporcionárselos a las RNNs. El objetivo principal del tutorial es iniciar a las personas en el uso de RNNs para tareas de clasificación de texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bSvctmWZoeg"
      },
      "source": [
        "<img src=\"https://ashutoshtripathicom.files.wordpress.com/2021/06/rnn-vs-lstm.png\">\n",
        "\n",
        "<center>Arquitectura de una Red Recurrente y una LSTM.</center>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wocYhOttEXIk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTzhaErEEXhy"
      },
      "source": [
        "### Cargar el dataset\n",
        "\n",
        "En esta sección, hemos cargaremos el conjunto de datos `AG_NEWS` para crear un vocabulario utilizando tokens generados a partir de ejemplos de texto del conjunto de datos. Posteriormente, el vocabulario se utilizará para mapear tokens a índices que se utilizarán para identificarlos. Estos índices generados para tokens de ejemplos de texto se darán como entrada a las redes neuronales para clasificar documentos de texto.\n",
        "\n",
        "Para esto debemos cargar los archivos de entrenamientos y testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdF80yyLfpF_"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/tutoriales/data/ag_news/train.csv\n",
        "!wget https://raw.githubusercontent.com/fvillena/dcc-ia-nlp/master/tutoriales/data/ag_news/test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij_TkglIEtG0"
      },
      "source": [
        "El `AG_NEWS` dataset, es un conjunto de datos que clasifica diferentes artículos de noticias de acuerdos a su contenidos, por lo que presenta la siguientes columnas:\n",
        "\n",
        "- El índice de la clase a la pertence el artículo.\n",
        "- El título del artículo.\n",
        "- La Descripción del artículo\n",
        "\n",
        "En el siguiente [link](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset) es posible encontrar más información del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJRVQCo4FkYF"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RWJZSjlG_ep"
      },
      "source": [
        "Para que `PyTorch` sea capaz tomar la información del dataset, es necesario cargarlo a través de un generador, para eso creamos la siguiente función que extrae la etiqueta y el contenido de la noticias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Hg1q9sahnhX"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset):\n",
        "  with open(dataset, encoding='utf-8') as dataset_file:\n",
        "    reader = csv.reader(dataset_file)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "      yield int(row[0]), f'{row[1]} {row[2]}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtqhW4tOh6Uc"
      },
      "outputs": [],
      "source": [
        "next(load_dataset('test.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuYiYXLnHj1v"
      },
      "source": [
        "Luego, ejecutamos la función tanto los datasets de `train` y `test`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xySHX1jXidXP"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset('train.csv')\n",
        "test_dataset = load_dataset('test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJgiK6xFT1YP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crg4LpUJT1fI"
      },
      "source": [
        "### Crear vocabulario\n",
        "\n",
        "Una vez que hemos cargado el dataset, es necesario crear nuestro vocabulario a partir de los tokens que componen nuestro dataset. Para esto, se deben seguir los siguientes pasos:\n",
        "\n",
        "- Es necesario tokenizar el dataset.\n",
        "- A partir del el dataset tokenizado, se crea el vocabulario.\n",
        "\n",
        "Para crear el vocabulario usaremos la API de `torchtext`, una librería de `PyTorch`, que contiene una API para trabajar usando métodos de deep learning para NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZfPOoxihWRu"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def build_vocabulary(datasets):\n",
        "    for dataset in datasets:\n",
        "        for _, text in dataset:\n",
        "            yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(build_vocabulary([train_dataset, test_dataset]), min_freq=1, specials=[\"<UNK>\"])\n",
        "\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO-uBIJGXwrr"
      },
      "source": [
        "Internamente, la estructura del vocabulario, le asigna un índice a token dentro del vocaluario. Como se ve en el siguiente anterior. Con esto es posible obtener todos los indice de una oración dentro del corpus de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAUaPGy3ir2a"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer(\"Hello how are you?, Welcome to CoderzColumn!!\")\n",
        "indexes = vocab(tokens)\n",
        "\n",
        "tokens, indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVJJeacGXaRt"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eecoyxNXWzZH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2U07qbFYCzO"
      },
      "source": [
        "### Cargar el `dataloader`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDxHe7At7gHN"
      },
      "source": [
        "Para crear el dataloader, debemos cargar los datos a una instancia de la clase `Dataset` de `PyTorch`, sin embargo podemos usar la función `to_map_style_dataset` para no implementar la clase completa. Ojo que el uso de la función depende caso, a veces es inevitable tener que implementar su propio wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDiz8DqqiyG9"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset('train.csv')\n",
        "test_dataset = load_dataset('test.csv')\n",
        "train_dataset, test_dataset  = to_map_style_dataset(train_dataset), to_map_style_dataset(test_dataset)\n",
        "\n",
        "target_classes = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "max_words = 25\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeA_ZuD27gHP"
      },
      "source": [
        "Dado que iremos transformando el dataset a medida que extrayendo cada `batch` de texto, es necesario definir una función que haga esta conversión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG03kQslIB8D"
      },
      "outputs": [],
      "source": [
        "def vectorize_batch(batch):\n",
        "    Y, X = list(zip(*batch))\n",
        "    X = [vocab(tokenizer(text)) for text in X]\n",
        "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
        "\n",
        "    return torch.tensor(X, dtype=torch.int32), torch.tensor(Y) - 1 ## We have deducted 1 from target names to get them in range [0,1,2,3] from [1,2,3,4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W09K3_2D7gHP"
      },
      "source": [
        "Luego, definimos nuestra instancia del `DataLoader`, este objeto será aquel que irá iterando y extrayendo cada `batch` del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEd4N8TIIEv8"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_batch, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset , batch_size=1024, collate_fn=vectorize_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sogh3fPZYXGz"
      },
      "source": [
        "### Definición Red Neuronal Recurrente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xej2Fa-i7gHQ"
      },
      "source": [
        "Previamente, a la definición de la Red Neuronal debemos definir los hiperparametros de está. No es una obligación, pero siempre es bueno hacerlo antes, para entender las dimensiones de nuestra red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HhbMGyp7gHR"
      },
      "outputs": [],
      "source": [
        "\n",
        "embed_len = 50\n",
        "hidden_dim = 50\n",
        "n_layers=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6JuNrCw7gHS"
      },
      "source": [
        "Para definir una Red Neuronal en `PyTorch`, debemos extender la clase `torch.Module`, en este caso `torch` fue renombrado como `nn`. Dentro de la constructor de la clase se definen todas las capas de la que nuestra red neuronal, en el caso de la tarea que buscamos resolver, se definen las siguientes:\n",
        "\n",
        "- Una capa de `Embedding`, para encodar cada una de las palabras del vocabulario a un vector denso.\n",
        "- Una capa de `RNN`, que implementa la arquitectura de `RNN`, por lo que no es necesario hacerlo a mano. Esta red agrega la secuencia de temporalidad de cada de las de palabras, considerando el contexto de la frase.\n",
        "- Una capa `Linear`, esta capa se encarga de resolver la tarea de clasificación que buscamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OI4FoxZKIQk6"
      },
      "outputs": [],
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
        "        self.rnn = nn.RNN(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, len(target_classes))\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn(embeddings, torch.randn(n_layers, len(X_batch), hidden_dim))\n",
        "        return self.linear(output[:,-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96kPSc5k7gHT"
      },
      "source": [
        "Definimos el clasificador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed7r6nyRjw2c"
      },
      "outputs": [],
      "source": [
        "rnn_classifier = RNNClassifier()\n",
        "\n",
        "rnn_classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMji0ttE7gHX"
      },
      "source": [
        "Para explorar las dimensiones de nuestras capas, podemos ejecutar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovVUcLVrj2Yb"
      },
      "outputs": [],
      "source": [
        "for layer in rnn_classifier.children():\n",
        "    print(\"Layer : {}\".format(layer))\n",
        "    print(\"Parameters : \")\n",
        "    for param in layer.parameters():\n",
        "        print(param.shape)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CNZDUPoYhhC"
      },
      "source": [
        "### Funciones de entrenamiento y Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0D2_Bgg7gHZ"
      },
      "source": [
        "Para extraer los valores de accuracy es necesario calcularlos a medida que se entrena el `batch`, para esto definimos una función que se encarga de entrenar en los modelo en cada iteración, guardando la loss y el accuracy en listas estandar de Python, para esto se implementan dos funciones:\n",
        "\n",
        "- `CalValLossAndAccuracy`: calcula la loss de la función de pérdida y guarda el accuracy que servirá para evaluar el modelo.\n",
        "- `TrainModel`: gestiona el entrenamiento del modelo en base a la cantidad de épocas escogidas por el usuario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVviKuJBj_1r"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
        "    with torch.no_grad():\n",
        "        Y_shuffled, Y_preds, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_shuffled.append(Y)\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_shuffled = torch.cat(Y_shuffled)\n",
        "        Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
        "    for i in range(1, epochs+1):\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds = model(X)\n",
        "\n",
        "            loss = loss_fn(Y_preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        CalcValLossAndAccuracy(model, loss_fn, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICSd1SSnYtAq"
      },
      "source": [
        "### Entrenamiento del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O58HZ8oi7gHb"
      },
      "source": [
        "Ejecutamos el entramiento del modelo, esto tardará algunos minutos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkCtjn6FkDP9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "epochs = 15\n",
        "learning_rate = 1e-3\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "rnn_classifier = RNNClassifier()\n",
        "optimizer = Adam(rnn_classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "TrainModel(rnn_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9pmh56FYywx"
      },
      "source": [
        "### Realizar prediciones de la Red entrenada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlm_ztFT7gHc"
      },
      "source": [
        "Para realizar las predicciones del modelo, es necesario evaluar su performance en el conjunto de entrenamiento. Para esto definimos la función `MakePredictions`, que calcula la loss del conjunto de testing y guarda las predicciones en una lista estandar de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8D3jAPgl2a_"
      },
      "outputs": [],
      "source": [
        "def MakePredictions(model, loader):\n",
        "    Y_shuffled, Y_preds = [], []\n",
        "    for X, Y in loader:\n",
        "        preds = model(X)\n",
        "        Y_preds.append(preds)\n",
        "        Y_shuffled.append(Y)\n",
        "    gc.collect()\n",
        "    Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
        "\n",
        "    return Y_shuffled.detach().numpy(), F.softmax(Y_preds, dim=-1).argmax(dim=-1).detach().numpy()\n",
        "\n",
        "Y_actual, Y_preds = MakePredictions(rnn_classifier, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iYTCO-K7gHd"
      },
      "source": [
        "Utlizamos las funciones de `scikit-learn` para generar el reporte de clasificación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL4P710cl50v"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKzyswnomk4Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_actual], [target_classes[i] for i in Y_preds],\n",
        "                                    normalize=True,\n",
        "                                    title=\"Confusion Matrix\",\n",
        "                                    cmap=\"Purples\",\n",
        "                                    hide_zeros=True,\n",
        "                                    figsize=(5,5)\n",
        "                                    );\n",
        "plt.xticks(rotation=90);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn3KD_1cZFGp"
      },
      "source": [
        "### Stackear muchas redes recurrentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa0CjAzO7gHg"
      },
      "source": [
        "Este código es similar al anterior, la diferencia es que variamos la cantidad de RNN utilizadas proponiendo una nueva arquitectura que podría mejorar o empeorar la performance del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SkUTBUjmnkY"
      },
      "outputs": [],
      "source": [
        "embed_len = 50\n",
        "hidden_dim1 = 50\n",
        "hidden_dim2 = 60\n",
        "hidden_dim3 = 75\n",
        "n_layers=1\n",
        "\n",
        "class StackingRNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StackingRNNClassifier, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
        "        self.rnn1 = nn.RNN(input_size=embed_len, hidden_size=hidden_dim1, num_layers=1, batch_first=True)\n",
        "        self.rnn2 = nn.RNN(input_size=hidden_dim1, hidden_size=hidden_dim2, num_layers=1, batch_first=True)\n",
        "        self.rnn3 = nn.RNN(input_size=hidden_dim2, hidden_size=hidden_dim3, num_layers=1, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim3, len(target_classes))\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings = self.embedding_layer(X_batch)\n",
        "        output, hidden = self.rnn1(embeddings, torch.randn(n_layers, len(X_batch), hidden_dim1))\n",
        "        output, hidden = self.rnn2(output, torch.randn(n_layers, len(X_batch), hidden_dim2))\n",
        "        output, hidden = self.rnn3(output, torch.randn(n_layers, len(X_batch), hidden_dim3))\n",
        "        return self.linear(output[:,-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA00NTIBJ8Gx"
      },
      "outputs": [],
      "source": [
        "rnn_classifier = StackingRNNClassifier()\n",
        "\n",
        "rnn_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZwmdMdyKBm3"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "epochs = 15\n",
        "learning_rate = 1e-3\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "rnn_classifier = StackingRNNClassifier()\n",
        "optimizer = Adam(rnn_classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "TrainModel(rnn_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6FBHssEKILK"
      },
      "outputs": [],
      "source": [
        "Y_actual, Y_preds = MakePredictions(rnn_classifier, test_loader)\n",
        "\n",
        "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds, target_names=target_classes))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jBo92HRKLiQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_actual], [target_classes[i] for i in Y_preds],\n",
        "                                    normalize=True,\n",
        "                                    title=\"Confusion Matrix\",\n",
        "                                    cmap=\"Purples\",\n",
        "                                    hide_zeros=True,\n",
        "                                    figsize=(5,5)\n",
        "                                    );\n",
        "plt.xticks(rotation=90);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1: Cambio de arquitecturas\n",
        "\n",
        "Postule al menos 2 hipótesis sobre como podrían afectar a los resultados cambios en la red con distintos arquitecturas RNN. Los cambios a probar son:\n",
        "\n",
        "- Pruebe cambiar la capa de RNN en el modelo original con otras arquitecturas como GRU  y LSTM. ¿Hay diferencias en los resultados? ¿A qué podría deberse?\n",
        "\n",
        "- Pruebe cambiar el número de capas con distintas arquitecturas RNN. Nuevamente, ¿observa cambios?\n",
        "\n"
      ],
      "metadata": {
        "id": "7kOfV99J1d86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hipótesis 1"
      ],
      "metadata": {
        "id": "BWHbZ8h66f4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hipótesis 2"
      ],
      "metadata": {
        "id": "HqFq8-lK6mII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P2: Más experimentos\n",
        "\n",
        "Pruebe a utilizar una red RNN para resolver algún otro problema de clasificación. Por ejemplo, podría usar el corpora que vimos en tutoriales anteriores relacionado a predecir si un texto corresponde a `dental` o `no_dental`.\n",
        "\n",
        "Recuerde realizar todos los cambios correspondientes en la red, incluidos por ejemplo:\n",
        "\n",
        "- Cambiar el output (por ejemplo en caso de que cambie el número de clases a predecir)\n",
        "- Cambiar la función de activación y la función de loss (por ejemplo para problemas de clasificación binaria)"
      ],
      "metadata": {
        "id": "fJy98W4d6pfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P3 (Opcional): Flujo de dato en la red\n",
        "\n",
        "Pruebe pasar un dato programando \"manualmente\" su flujo a través de la red original (el flujo que sigue la función `predict`). Seleccione un dato al azar del corpus y realice las operaciones matemáticas correspondientes para predecir su clase correspondiente.\n",
        "\n",
        "Puede utilizar las librerías que necesite para realizar dichas operaciones.\n",
        "\n",
        "¡Hacer este ejercicio le puede ayudar a tener una mejor comprensión de las operaciones aplicadas a cada dato!"
      ],
      "metadata": {
        "id": "GJILdqzp9TBT"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}