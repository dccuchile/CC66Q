{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZb5iCp2593d"
      },
      "source": [
        "# Preprocesamiento de datos de texto"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download es_core_news_sm # Instalaci√≥n Modelo Spacy de procesamiento de lenguaje natural en espa√±ol\n",
        "!pip install -U \"datasets<3.0\" #Con el fin de poder cargar el dataset de ejemplo"
      ],
      "metadata": {
        "id": "0pEcN8Q76A9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyePdRUm593g"
      },
      "outputs": [],
      "source": [
        "import datasets  # Biblioteca de manejo de conjuntos de datos para procesamiento de lenguaje natural\n",
        "import es_core_news_sm  # Modelo Spacy de procesamiento de lenguaje natural en espa√±ol\n",
        "import spacy  # Biblioteca de procesamiento de lenguaje natural\n",
        "import pandas as pd  # Biblioteca de manejo de conjuntos de datos\n",
        "import re  # M√≥dulo de expresiones regulares\n",
        "import tokenizers  # Biblioteca de tokenizaci√≥n de texto\n",
        "import nltk  # Biblioteca de procesamiento de lenguaje natural\n",
        "from pathlib import Path  # Biblioteca para manejo de paths relativos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFkCcBKH593i"
      },
      "source": [
        "## ü§ó Datasets\n",
        "\n",
        "ü§ó (HuggingFace) Datasets es una biblioteca de manejo de conjuntos de datos para procesamiento de lenguaje natural que se destaca por la simplicidad de sus m√©todos y el gran repositorio ü§ó Hub que contiene muchos conjuntos de datos libres para descargar s√≥lo con una linea de Python.\n",
        "\n",
        "En nuestro curso trabajaremos con `spanish_diagnostics`, un conjunto de datos de nuestro grupo investigaci√≥n PLN@CMM que contiene textos de sospechas diagn√≥sticas de la lista de espera chilena y est√° etiquetado con el destino de la interconsulta; este destino puede ser `dental` o `no_dental`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8lUmYJ4593j"
      },
      "outputs": [],
      "source": [
        "# Con esta linea descargamos el conjunto de datos completo\n",
        "spanish_diagnostics = datasets.load_dataset('fvillena/spanish_diagnostics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGKQLLk1593k"
      },
      "outputs": [],
      "source": [
        "def normalize(text, remove_tildes=True):\n",
        "    \"\"\"Normaliza una cadena de texto convirti√©ndo todo a min√∫sculas, quitando los caracteres no alfab√©ticos y los tildes\"\"\"\n",
        "    text = text.lower()  # Llevamos todo a min√∫scula\n",
        "    # Reemplazamos los caracteres no alfab√©ticos por un espacio\n",
        "    text = re.sub(r'[^A-Za-z√±√°√©√≠√≥√∫]', ' ', text)\n",
        "    if remove_tildes:\n",
        "        text = re.sub('√°', 'a', text)  # Reemplazamos los tildes\n",
        "        text = re.sub('√©', 'e', text)\n",
        "        text = re.sub('√≠', 'i', text)\n",
        "        text = re.sub('√≥', 'o', text)\n",
        "        text = re.sub('√∫', 'u', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pChRUOUI593l"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized = spanish_diagnostics[\"train\"].map(\n",
        "    lambda x: {  # Utilizamos una funci√≥n an√≥nima que devuelve un diccionario\n",
        "        # Esta es una nueva caracter√≠stica que agregaremos a nuestro conjunto de datos.\n",
        "        \"normalized_text\": normalize(x[\"text\"])\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL3d-_B5593m"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyqO4nxM593n"
      },
      "source": [
        "Ahora nuestro conjunto de datos cuenta con una nueva caracter√≠stica `normalized_text`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnE9g4i3593n"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1gQ3z0w593o"
      },
      "source": [
        "## Tokenizaci√≥n\n",
        "\n",
        "La tokenizaci√≥n es el proceso de demarcaci√≥n de secciones de una cadena de caracteres. Estas secciones podr√≠an ser oraciones, palabras o subpalabras.\n",
        "\n",
        "El m√©todo m√°s simple para tokenizar una cadena de caracteres en nuestro lenguaje es la separaci√≥n por espacios. Aplicamos una separaci√≥n por espacios mediante el m√©todo `str.split()` sobre nuestro conjunto de datos normalizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EoqQbAZ593o"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized[0][\"normalized_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Lqj37vj593o"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized[0][\"normalized_text\"].split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGl9eT3P593p"
      },
      "source": [
        "Si bien el m√©todo de separaci√≥n por espacios funciona bien en nuestro conjunto de datos normalizado, tambi√©n quisi√©ramos tokenizar nuestro texto sin normalizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzZAnvsd593p"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized[0][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__mIlxJh593p"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized[0][\"text\"].split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bR4PBuA593p"
      },
      "source": [
        "Al aplicar el mismo m√©todos podemos observar que no funciona totalmente bien debido a la presencia de caracteres no alfab√©ticos. Para solucionar esto, existen m√©todos basados en una serie de reglas para solucionar estos problemas. Utilizaremos la implementaci√≥n de un tokenizador basado en reglas de la biblioteca de procesamiento de lenguaje natural Spacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQsIb2KU593p"
      },
      "outputs": [],
      "source": [
        "spacy_tokenizer = es_core_news_sm.load().tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0TmifRY593q"
      },
      "outputs": [],
      "source": [
        "list(spacy_tokenizer(spanish_diagnostics_normalized[0][\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwBf6Ssa593q"
      },
      "source": [
        "Al utilizar el tokenizador basado en reglas, podemos tener resultados mucho mejores que los anteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1njMY1hX593q"
      },
      "source": [
        "### ü§ó Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S836gibE593q"
      },
      "source": [
        "ü§ó tambi√©n cuenta con una biblioteca llamada Tokenizers, con la cual podemos construir nuestro tokenizador basado en nuestro conjunto de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFTVM5B3593q"
      },
      "source": [
        "Instanciamos el tokenizador con un modelo WordPiece, el cual parte construyendo un vocabulario que incluye todas los caracteres presentes en el conjunto de datos y posteriormente comienza a mezclar caracteres hasta encontrar conjuntos de caracteres que tienen m√°s probabilidad de aparecer juntos que separados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuLYzh9F593q"
      },
      "outputs": [],
      "source": [
        "tokenizer = tokenizers.Tokenizer(tokenizers.models.WordPiece())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9g0vGqs593q"
      },
      "source": [
        "Esta biblioteca nos permite a√±adir pasos de normalizaci√≥n directamente. Replicamos lo mismo que hacemos con nuestra funci√≥n `normalizer()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTFJ8Qvt593r"
      },
      "outputs": [],
      "source": [
        "normalizer = tokenizers.normalizers.Sequence([\n",
        "    tokenizers.normalizers.Lowercase(),  # Llevamos todo a min√∫scula\n",
        "    # Separamos cada caracter seg√∫n los elementos que lo componen: √° -> (a, ¬¥)\n",
        "    tokenizers.normalizers.NFD(),\n",
        "    tokenizers.normalizers.StripAccents(),  # Eliminamos todos los acentos\n",
        "    # Reemplazamos todos los caracteres no alfab√©ticos\n",
        "    tokenizers.normalizers.Replace(tokenizers.Regex(r\"[^a-z ]\"), \" \")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIP3Rf5Z593r"
      },
      "outputs": [],
      "source": [
        "normalizer.normalize_str(spanish_diagnostics_normalized[0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLH68o3b593r"
      },
      "source": [
        "A√±adimos el normalizador al tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhrUpxqE593r"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWtwxT4z593r"
      },
      "source": [
        "Pre tokenizamos nuestro conjunto de datos mediante espacio para delimitar el tama√±o que puede tener cada token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bi8Zu8A593r"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H5THAkv593r"
      },
      "source": [
        "Instanciamos el entrenador que entrenar√° nuestro tokenizador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yICliA9593r"
      },
      "outputs": [],
      "source": [
        "trainer = tokenizers.trainers.WordPieceTrainer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWGj68NX593s"
      },
      "source": [
        "Entrenamos el tokenizador sobre nuestro conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SN9gg09593s"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(spanish_diagnostics_normalized[\"text\"], trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKnWYs0y593s"
      },
      "source": [
        "Mediante el m√©todo `Tokenizer.encode()` obtenemos la representaci√≥n tokenizada de nuesto texto. Esta representaci√≥n contiene varios atributos, donde los m√°s interesantes son:\n",
        "\n",
        "- `ids`: Contiene nuestro texto representado a trav√©s de una lista que contiene los identificadores de cada token.\n",
        "- `tokens`: Contiene nuestro texto representado a trav√©s de una lista que contiene el texto de cada token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SRvOHZf593s"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized[0][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H46SVkcS593s"
      },
      "outputs": [],
      "source": [
        "tokenized_output = tokenizer.encode(spanish_diagnostics_normalized[0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxSN_5eP593t"
      },
      "outputs": [],
      "source": [
        "# Para ver todo el vocabulario del tonekizador consultar la siguiente funcion\n",
        "tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0GIB9UX593t"
      },
      "outputs": [],
      "source": [
        "tokenized_output.ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJR6gK1s5932"
      },
      "outputs": [],
      "source": [
        "tokenized_output.tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vam2APe5933"
      },
      "source": [
        "Tal como lo hicimos anteriormente podemos aplicar paralelamente nuestro tokenizador sobre el conjunto de datos mediante el m√©todo `Dataset.map()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9LG2vtf5933"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized_tokenized = spanish_diagnostics_normalized.map(\n",
        "    lambda x: {\"tokenized_text\": tokenizer.encode(x[\"text\"]).tokens})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLMU1zY_5933"
      },
      "source": [
        "Nuestro conjunto de datos ahora contiene el texto tokenizado en la caracter√≠stica `tokenized_text`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-FyS9Dc5933"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized_tokenized[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TWw6xUJ5933"
      },
      "source": [
        "## Stemming y Lematizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pdo3UQu5933"
      },
      "source": [
        "Con el fin de disminuir la cantidad de caracter√≠sticas de las representaciones de texto existen m√©todos que reducen el tama√±o de vocabulario al eliminar inflexiones que puedan tener las palabras. Estos m√©todos son:\n",
        "\n",
        "- Lematizaci√≥n: Este m√©todo lleva una palabra en su forma flexionada a su forma base, por ejemplo *tratada* -> *tratar*\n",
        "- Stemming: Este m√©todo trunca las palabras de entrada mediante un algoritmo predefinido para encontrar la ra√≠z de la misma, por ejemplo *tratada* -> *trat*\n",
        "\n",
        "El proceso de lematizaci√≥n lo haremos a trav√©s de la biblioteca Spacy y el proceso de stemming a trav√©s de la biblioteca NLTK utilizando el algoritmo Snowball."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-eMhGUO5934"
      },
      "source": [
        "Instanciamos el analizador de Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW6S71nJ5934"
      },
      "outputs": [],
      "source": [
        "nlp = es_core_news_sm.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwZOlQmu5934"
      },
      "source": [
        "Definimos como tokenizador el que entrenamos anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tmeKRz-5934"
      },
      "outputs": [],
      "source": [
        "def custom_tokenizer(text):\n",
        "    tokens = tokenizer.encode(text).tokens\n",
        "    return spacy.tokens.Doc(nlp.vocab, tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBKnIt6A5934"
      },
      "outputs": [],
      "source": [
        "nlp.tokenizer = custom_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRo1G2Xi5934"
      },
      "source": [
        "Instanciamos el Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUjHTJBs5934"
      },
      "outputs": [],
      "source": [
        "stemmer = nltk.stem.SnowballStemmer(\"spanish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY_DL5T-5935"
      },
      "source": [
        "Podemos verificar c√≥mo funcionan estos m√©todos sobre un texto de prueba de nuestro conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2OaKE3t5935"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized_tokenized[5][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6jhbmgS5935"
      },
      "outputs": [],
      "source": [
        "for t in nlp(spanish_diagnostics_normalized_tokenized[5][\"text\"]):\n",
        "    print(\n",
        "        f\"Token: {t.text}\\nLema: {t.lemma_}\\nStem: {stemmer.stem(t.text)}\\n---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}