{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuCsNvgKahNZ"
      },
      "source": [
        "# Document representation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"datasets<3.0\""
      ],
      "metadata": {
        "id": "K5o3lknla39m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElsAQ4EVahNb"
      },
      "outputs": [],
      "source": [
        "import sklearn.feature_extraction # Módulo de sklearn para extracción de características\n",
        "import datasets # Biblioteca de manejo de conjuntos de datos para procesamiento de lenguaje natural\n",
        "import numpy as np # Biblioteca de manejo de datos vectoriales\n",
        "import pandas as pd # Biblioteca de manejo de conjuntos de datos\n",
        "import spacy.lang.es # Biblioteca de procesamiento de lenguaje natural\n",
        "import matplotlib.pyplot as plt # Biblioteca de visualización\n",
        "import sklearn.pipeline # Módulo de sklearn para el desarrollo de flujos de trabajo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeLwxQd2ahNe"
      },
      "source": [
        "Cargamos el conjunto de datos del curso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO-FwTk7ahNe"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics = datasets.load_dataset('fvillena/spanish_diagnostics') # Cargamos las particiones de entrenamiento y prueba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLFRvt7PahNf"
      },
      "source": [
        "Cargamos una lista de stopwords desde la biblioteca Spacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvQvYLtBahNf"
      },
      "outputs": [],
      "source": [
        "stopwords = spacy.lang.es.stop_words.STOP_WORDS # La biblioteca Spacy tiene una lista de stopwords en español"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kvAAVheahNg"
      },
      "source": [
        "## Representación de documentos\n",
        "\n",
        "Para poder trabajar con datos de texto, estos deben ser representados de una manera que pueda ser interpretada por los algoritmos de minería de texto. Típicamente se desea llegar a una matriz que tenga tantas filas como documentos tenga nuestro corpus y tantas columnas como características fueron extraídas desde el texto.\n",
        "\n",
        "Revisaremos 2 métodos de extracción de características:\n",
        "\n",
        "* Bag-of-words: Este método extrae la frecuencia de aparición de cada una de las palabras del documento y representa un documento como un vector de tantas dimensiones como palabras tenga el vocabulario.\n",
        "\n",
        "* Term frequency - inverse document frequency (TF-IDF): Este método extrae la frecuencia de aparición de cada una de las palabras y la multiplica por el inverso de la frecuencia de aparición de la palabra en todos los documentos. También se representa cada documento como un vector de tantas dimensiones como palabras tenga el vocabulario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhEke2V8ahNh"
      },
      "source": [
        "### Bag of Words\n",
        "\n",
        "es una técnica utilizada en el procesamiento del lenguaje natural para representar textos como conjuntos no ordenados de palabras, ignorando la estructura gramatical y el orden de las palabras en el texto. En este enfoque, cada documento se representa mediante un vector que cuenta la ocurrencia o frecuencia de las palabras en el documento.\n",
        "\n",
        "El proceso de creación de una representación de Bolsa de Palabras consta de los siguientes pasos:\n",
        "\n",
        "* Tokenización: El texto se divide en unidades más pequeñas, generalmente palabras. Cada palabra se considera un \"token\".\n",
        "\n",
        "* Creación del vocabulario: Se construye un vocabulario único a partir de todos los tokens en el conjunto de documentos. Cada palabra del vocabulario se asigna a un índice único.\n",
        "\n",
        "* Creación del vector de características: Para cada documento, se crea un vector de características que representa la frecuencia de cada palabra del vocabulario en el documento. El valor en cada posición del vector corresponde a la frecuencia de esa palabra en el documento.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hLvya7MXjsSc3NS2SoLMEg.png\" alt=\"medium\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8JxiRgUahNh"
      },
      "source": [
        "Instanciamos un extractor de características Bag-of-words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRaWrNrdahNi"
      },
      "outputs": [],
      "source": [
        "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
        "    stop_words = list(stopwords), # Le pasamos la lista de stopwords para eliminarlas del vocabulario\n",
        "    max_df = 0.05, # Eliminamos del vocabulario el 5% de palabras más frecuentes (stopwords específicas del corpus)\n",
        "    min_df = 2 # Eliminamos del vocabulario las palabras que tienen una frecuencia menor a 2 (típicamente palabras malformadas)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HwrWdy_ahNj"
      },
      "source": [
        "Ajustamos el vectorizador sobre los textos del conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crXU__RTahNj"
      },
      "outputs": [],
      "source": [
        "count_vectorizer.fit(spanish_diagnostics[\"train\"][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHbnS3ZrahNk"
      },
      "source": [
        "Exploraremos cómo está representando nuestros documentos este vectorizador.\n",
        "\n",
        "Este es un texto de ejemplo del corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_KHLiIDahNk"
      },
      "outputs": [],
      "source": [
        "text_vectorized = count_vectorizer.transform(spanish_diagnostics[\"train\"][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QLvEeAAahNk"
      },
      "outputs": [],
      "source": [
        "text_vectorized.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtJ0QdU7ahNl"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics[\"train\"][\"text\"][69983]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK3myB6qahNl"
      },
      "outputs": [],
      "source": [
        "count_vectorizer.transform([spanish_diagnostics[\"train\"][\"text\"][69983]]).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CibEPK6LahNl"
      },
      "outputs": [],
      "source": [
        "def get_word_scores(text,vectorizer):\n",
        "    \"\"\"A partir de un texto y un vectorizador retorna los puntajes asignados a cada palabra del texto\"\"\"\n",
        "    feature_names = list({k: v for k, v in sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])}.keys()) # Vocabulario\n",
        "    doc = vectorizer.transform([text]) # Vectorizamos el texto de entrada\n",
        "    idxs = np.argwhere(doc)[:,1] # Extraemos los índices donde sí hay palabras representadas\n",
        "    words = [feature_names[i] for i in idxs] # Extraemos las palabras asociadas a los índices extraídos\n",
        "    scores = np.array(doc.todense())[0][idxs] # Extraemos los puntajes asociadas a los índices extraídos\n",
        "    return list(reversed(sorted(zip(words,scores),key=lambda tup: tup[1]))) # Retornamos una lista de palabras y puntajes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_hWpmYeahNm"
      },
      "source": [
        "Vemos que nuestro vectorizador le dio más peso a la palabra caries de nuestro documento porque es la palabra más frecuente y a a un grupo de palabras les asignó el mismo puntaje 1 porque cada una aprece 1 vez."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmzNqdNDahNm"
      },
      "outputs": [],
      "source": [
        "get_word_scores(spanish_diagnostics[\"train\"][\"text\"][69983],count_vectorizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx9nSWtQahNm"
      },
      "source": [
        "### TF-IDF\n",
        "\n",
        "Es una técnica utilizada en el procesamiento del lenguaje natural para ponderar la importancia de un término (palabra) en un documento dentro de una colección de documentos. Consiste en dos componentes principales: TF (Frecuencia del Término) e IDF (Frecuencia Inversa del Documento).\n",
        "\n",
        "* Frecuencia del Término (TF): La componente TF mide la frecuencia relativa de un término en un documento específico. El objetivo es asignar un mayor peso a los términos que aparecen con más frecuencia dentro del documento, ya que se considera que estos términos son más importantes.\n",
        "La fórmula básica para calcular la TF es: $$TF = \\frac{(\\text{Número de veces que aparece el término en el documento})}{(\\text{Número total de términos en el documento})}$$\n",
        "\n",
        "    Ejemplo de TF: Supongamos que tenemos un documento que contiene la siguiente frase: \"El perro juega en el parque\". Si queremos calcular la frecuencia del término \"perro\" en este documento, contaríamos que aparece una vez. Si asumimos que hay un total de cinco términos en el documento, entonces la frecuencia del término \"perro\" sería $\\frac{1}{5}$ = 0.2.\n",
        "\n",
        "\n",
        "* Frecuencia Inversa del Documento (IDF): La componente IDF mide la importancia de un término en el contexto de una colección de documentos. Se basa en la suposición de que los términos menos comunes en la colección pueden ser más informativos que los términos comunes. La fórmula básica para calcular el IDF es: $$IDF = log(\\frac{\\text{Número total de documentos en la colección}}{\\text{Número de documentos que contienen el término}})$$\n",
        "\n",
        "    El IDF se calcula como el logaritmo del cociente entre el número total de documentos y el número de documentos que contienen el término dado. El logaritmo se aplica para suavizar la escala del IDF y evitar una sobrevaloración de términos muy raros.\n",
        "\n",
        "    Ejemplo de IDF: Supongamos que tenemos una colección de documentos que contiene un total de 100 documentos. Si el término \"perro\" aparece en 50 de esos documentos, entonces el IDF se calcularía como log(100/50) = log(2) ≈ 0.301.\n",
        "\n",
        "\n",
        "* TF-IDF: El TF-IDF combina la información de TF e IDF para calcular un peso final para cada término en un documento. Se calcula multiplicando la TF del término en el documento por su IDF en la colección. La fórmula para calcular el TF-IDF es: $$TF-IDF = TF * IDF$$\n",
        "\n",
        "    El TF-IDF aumentará para los términos que aparezcan con frecuencia en un documento específico (alta TF) y sean menos comunes en la colección en su conjunto (alta IDF).\n",
        "\n",
        "    Ejemplo de TF-IDF: Supongamos que queremos calcular el TF-IDF para el término \"perro\" en un documento específico. Si el valor de TF es 0.2 (como en el ejemplo anterior) y el valor de IDF es 0.301 (como en el ejemplo IDF), entonces el valor de TF-IDF sería 0.2 * 0.301 = 0.0602."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO74RmY_ahNn"
      },
      "source": [
        "Ajustamos un vectorizador que utiliza TF-IDF y lo ajustamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRlz-5JcahNn"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
        "    stop_words = list(stopwords),\n",
        "    max_df = 0.05,\n",
        "    min_df = 2\n",
        ")\n",
        "tfidf_vectorizer.fit(spanish_diagnostics[\"train\"][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaSaZck7ahNn"
      },
      "source": [
        "Podemos observar que caries sigue siendo la palabra con el mayor puntaje. Pero vemos que todas las palabras tienen un puntaje distinto, en donde destacamos que la palabra fundamento tiene el menor puntaje. Intuitivamente podemos darnos cuenta que esta representación es mejor porque la palabra fundamento no nos aporta mucha información en el documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGhToWZFahNn"
      },
      "outputs": [],
      "source": [
        "get_word_scores(spanish_diagnostics[\"train\"][\"text\"][69983],tfidf_vectorizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf7NxEAIahNo"
      },
      "source": [
        "Vectorizamos los textos de nuestros conjuntos de entrenamiento y prueba. con el método CountVectorizer.transform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifMY930GahNo"
      },
      "outputs": [],
      "source": [
        "text_vectorized_train = count_vectorizer.transform(spanish_diagnostics[\"train\"][\"text\"])\n",
        "text_vectorized_test = count_vectorizer.transform(spanish_diagnostics[\"test\"][\"text\"])\n",
        "feature_names = list({k: v for k, v in sorted(count_vectorizer.vocabulary_.items(), key=lambda item: item[1])}.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFsjvEm_ahNo"
      },
      "source": [
        "La forma de nuestra matriz es de (cantidad de documentos en el conjunto, tamaño del vocabulario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5TwR6ZsrahNo"
      },
      "outputs": [],
      "source": [
        "text_vectorized_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlhYR6O8ahNo"
      },
      "outputs": [],
      "source": [
        "text_vectorized_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYt0RCfcahNp"
      },
      "source": [
        "## Similaridad entre documentos\n",
        "La similaridad entre documentos es una medida que nos permite cuantificar cuán similares son dos documentos entre sí. Existen varias métricas para calcular la similaridad, pero una de las más comunes es el coseno de la similitud.\n",
        "\n",
        "El coseno de la similitud se basa en el ángulo entre dos vectores en un espacio vectorial. Se calcula como el coseno del ángulo entre los vectores que representan los documentos. La fórmula para calcular la similaridad del coseno es:\n",
        "$$\\text{Similitud del coseno} = \\frac{A \\cdot B}{||A|| \\cdot ||B||}$$\n",
        "donde $A$ y $B$ son los vectores que representan los documentos, $(A \\cdot B)$ es el producto punto entre los dos vectores, y $||A||$ y $||B||$ son las normas (longitudes) de los vectores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjD9DrY5ahNp"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Calcula la similitud coseno entre dos vectores\"\"\"\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHWjFl4JahNp"
      },
      "source": [
        "Podemos utilizar la similaridad del coseno para comparar documentos y determinar cuán similares son entre sí. Un valor de similaridad del coseno cercano a 1 indica que los documentos son muy similares, mientras que un valor cercano a 0 indica que son poco similares.\n",
        "\n",
        "Usemos esta función para generar un buscador en donde podamos buscar documentos similares a un texto de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBlolfysahNp"
      },
      "outputs": [],
      "source": [
        "def search(query, top_n=5):\n",
        "    \"\"\"Busca los textos más similares al texto de entrada\"\"\"\n",
        "    query_vector = count_vectorizer.transform([query]).toarray()[0] # Vectorizamos el texto de entrada\n",
        "    similarities = []\n",
        "    for v in text_vectorized_train.toarray():\n",
        "        similarities.append(cosine_similarity(query_vector, v)) # Calculamos la similitud coseno entre el texto de entrada y cada texto del conjunto de entrenamiento\n",
        "    #remove nan similarities\n",
        "    similarities = np.nan_to_num(np.array(similarities)) # Convertimos la lista de similitudes a un array de numpy y reemplazamos los valores NaN por 0\n",
        "    top_indices = np.argsort(similarities)[-top_n:][::-1] # Obtenemos los índices de los textos más similares\n",
        "    results = []\n",
        "    for i in top_indices:\n",
        "        results.append((spanish_diagnostics[\"train\"][\"text\"][i], similarities[i])) # Agregamos el texto y su puntaje de similitud\n",
        "    return results # Retornamos los textos y sus puntajes de similitud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR9YoubiahNq"
      },
      "outputs": [],
      "source": [
        "search(\"paciente con pericoronaritis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYMUVM_KahNq"
      },
      "source": [
        "## Spoiler: Pipelines de Sklearn\n",
        "\n",
        "Los pipelines en la biblioteca scikit-learn son una forma conveniente y eficiente de encadenar múltiples pasos de procesamiento de datos y modelos de aprendizaje automático en un flujo de trabajo coherente. Un pipeline de scikit-learn combina transformadores y estimadores en una secuencia ordenada, donde los datos se pasan secuencialmente a través de cada etapa para su procesamiento.\n",
        "\n",
        "En scikit-learn, un transformador es cualquier objeto que implementa los métodos fit() y transform(). Los transformadores se utilizan para realizar transformaciones en los datos, como preprocesamiento, extracción de características o reducción de dimensionalidad. Por otro lado, un estimador es cualquier objeto que implementa los métodos fit() y predict(). Los estimadores se utilizan para ajustar modelos a los datos y realizar predicciones.\n",
        "\n",
        "El pipeline de scikit-learn permite definir una secuencia de pasos, donde cada paso es un par formado por un nombre y un transformador o estimador. Los datos se pasan a través de los pasos en el orden especificado, y cada paso toma los datos de entrada, realiza su operación y pasa los datos transformados al siguiente paso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEBvSE4DahNq"
      },
      "outputs": [],
      "source": [
        "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
        "    stop_words = list(stopwords), # Le pasamos la lista de stopwords para eliminarlas del vocabulario\n",
        "    max_df = 0.05, # Eliminamos del vocabulario el 5% de palabras más frecuentes (stopwords específicas del corpus)\n",
        "    min_df = 2 # Eliminamos del vocabulario las palabras que tienen una frecuencia menor a 2 (típicamente palabras malformadas)\n",
        ")\n",
        "\n",
        "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
        "    stop_words = list(stopwords),\n",
        "    max_df = 0.05,\n",
        "    min_df = 2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYXX16zzahNq"
      },
      "outputs": [],
      "source": [
        "pipe_bow = sklearn.pipeline.Pipeline([\n",
        "        ('vectorizer', count_vectorizer),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZBmmOqlahNq"
      },
      "outputs": [],
      "source": [
        "pipe_bow.fit_transform(spanish_diagnostics[\"train\"][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alD9vvNKahNw"
      },
      "outputs": [],
      "source": [
        "get_word_scores(spanish_diagnostics[\"train\"][\"text\"][69983],pipe_bow.named_steps['vectorizer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEMhmDucahNx"
      },
      "outputs": [],
      "source": [
        "pipe_tf_idf = sklearn.pipeline.Pipeline([\n",
        "        ('vectorizer', tfidf_vectorizer),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_eQYTGUahNx"
      },
      "outputs": [],
      "source": [
        "pipe_tf_idf.fit_transform(spanish_diagnostics[\"train\"][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVC9cz6_ahNx"
      },
      "outputs": [],
      "source": [
        "get_word_scores(spanish_diagnostics[\"train\"][\"text\"][69983],pipe_tf_idf.named_steps['vectorizer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdkTlwjcahNy"
      },
      "source": [
        "¿se puede meter mas pasos a un pipeline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLvi00ArahNy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "pipe = sklearn.pipeline.Pipeline([\n",
        "        ('vectorizer', vectorizer), #preprocesamiento\n",
        "        ('classifier', sklearn.naive_bayes.MultinomialNB()) #clasificador\n",
        "    ])\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}