{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oXoj_3yVaW2"
      },
      "source": [
        "# Statistics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"datasets<3.0\" #Con el fin de poder cargar el dataset de ejemplo"
      ],
      "metadata": {
        "id": "hx7vANK8V5QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj_S7khiVaW4"
      },
      "outputs": [],
      "source": [
        "import datasets  # Biblioteca de manejo de conjuntos de datos para procesamiento de lenguaje natural\n",
        "import re  # Biblioteca de expresiones regulares\n",
        "import collections  # Módulo de tipos de datos adicionales de Python\n",
        "import itertools  # Módulo de herramientas de manejo de iterables\n",
        "import matplotlib.pyplot as plt  # Biblioteca de visualización\n",
        "import wordcloud  # Biblioteca de visualización de nubes de palabras\n",
        "import PIL  # Biblioteca de manejo de imágenes\n",
        "import numpy as np  # Biblioteca de manejo de datos vectoriales\n",
        "import spacy.lang.es # Biblioteca de procesamiento de lenguaje natural en español"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoNzEU5dVaW6"
      },
      "source": [
        "Cargaremos y analizaremos nuestro conjunto de datos y lo compararemos con otro de opiniones de películas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnPa1I-DVaW7"
      },
      "outputs": [],
      "source": [
        "# Cargamos las particiones de entrenamiento y prueba\n",
        "spanish_diagnostics = datasets.load_dataset(\n",
        "    'fvillena/spanish_diagnostics', split=\"train+test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw7SLQb7VaW8"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyAQ8ueQVaW-"
      },
      "source": [
        "Esta es una de las instancias de nuestro conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_4ID8xaVaW_"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics[2][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTaakZaRVaW_"
      },
      "source": [
        "Normalizaremos nuestro texto con la función que programamos la clase anterior y tokenizaremos el texto mediante espacio."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Definimos una función para normalizar texto\n",
        "def normalize(text, remove_tildes = True):\n",
        "    \"\"\"Normaliza una cadena de texto convirtiéndo todo a minúsculas, quitando los caracteres no alfabéticos y los tildes\"\"\"\n",
        "    text = text.lower() # Llevamos todo a minúscula\n",
        "    text = re.sub(r'[^A-Za-zñáéíóú]', ' ', text) # Reemplazamos los caracteres no alfabéticos por un espacio\n",
        "    if remove_tildes:\n",
        "        text = re.sub('á', 'a', text) # Reemplazamos los tildes\n",
        "        text = re.sub('é', 'e', text)\n",
        "        text = re.sub('í', 'i', text)\n",
        "        text = re.sub('ó', 'o', text)\n",
        "        text = re.sub('ú', 'u', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "3JiaVK6xZACc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SowjaB7JVaW_"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized = spanish_diagnostics.map(\n",
        "    lambda x: {\n",
        "        \"normalized_text\": normalize(x[\"text\"])\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhSM0fI5VaXA"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized_tokenized = spanish_diagnostics_normalized.map(\n",
        "    lambda x: {\n",
        "        \"tokenized_text\": x[\"normalized_text\"].split()\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyuNZeHlVaXA"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3nx6ZbhVaXA"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_normalized_tokenized[\"tokenized_text\"][:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUaP_dC2VaXB"
      },
      "source": [
        "El conjunto de datos con el cual nos compararemos es un corpus de chistes en español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msJ0kh_VVaXB"
      },
      "outputs": [],
      "source": [
        "# Cargamos el dataset de Muchocine\n",
        "chistes = datasets.load_dataset('mrm8488/CHISTES_spanish_jokes', split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQghNL1VVaXB"
      },
      "outputs": [],
      "source": [
        "chistes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVVyn9yDVaXC"
      },
      "outputs": [],
      "source": [
        "chistes_normalized = chistes.map(\n",
        "    lambda x: {  # Utilizamos una función anónima que devuelve un diccionario\n",
        "        # Esta es una nueva característica que agregaremos a nuestro conjunto de datos.\n",
        "        \"normalized_text\": normalize(x[\"text\"])\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUWcgHXAVaXC"
      },
      "outputs": [],
      "source": [
        "chistes_normalized_tokenized = chistes_normalized.map(\n",
        "    lambda x: {\n",
        "        \"tokenized_text\": x[\"normalized_text\"].split()\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3rRKzCkVaXC"
      },
      "outputs": [],
      "source": [
        "chistes_normalized_tokenized[\"tokenized_text\"][0][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEYfwIWyVaXC"
      },
      "source": [
        "Un *corpus* (plural *corpora*) es el nombre que reciben los conjuntos de datos de texto, estos *corpora* se componen de documentos, los cuales son cada una de las instancias de texto de nuestro *corpus*. Una de las métricas de descripción de *corpora* es el número de documentos, o sea, la cantidad de instancias que tiene mi conjunto de datos.\n",
        "\n",
        "El número de documentos del *corpus* spanish_diagnostics es mucho mayor al *corpus* muchocine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdHHnbQvVaXC"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_n_documents = len(spanish_diagnostics)\n",
        "spanish_diagnostics_n_documents  # Número de documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXdFHWHDVaXD"
      },
      "outputs": [],
      "source": [
        "chistes_n_documents = len(chistes)\n",
        "chistes_n_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojoZFR2GVaXD"
      },
      "source": [
        "A nivel de tokens en nuestro corpus se pueden describir principalmente el número de tokens, el cual es la cantidad total de tokens que presenta nuesto corpus y el tamaño del voculario, el cual es la cantidad de tokens distintos que tiene nuestro corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9tz_b6IVaXD"
      },
      "source": [
        "Utilizaremos la clase collections.Counter para contar las apariciones de cada una de las palabras del vocabulario en los corpora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp17qeFWVaXE"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_words = collections.Counter(  # Con esta clase contamos cada una de las apariciones de las palabras\n",
        "    itertools.chain(  # Con esta función aplanamos nuestra lista anidada de tokens\n",
        "        *spanish_diagnostics_normalized_tokenized[\"tokenized_text\"]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1rF3JFiVaXE"
      },
      "outputs": [],
      "source": [
        "chistes_words = collections.Counter(itertools.chain(\n",
        "    *chistes_normalized_tokenized[\"tokenized_text\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZbcwM6NVaXE"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_n_tokens = sum(\n",
        "    [count for word, count in spanish_diagnostics_words.items()])\n",
        "spanish_diagnostics_n_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvXgCL3jVaXE"
      },
      "outputs": [],
      "source": [
        "chistes_n_tokens = sum([count for word, count in chistes_words.items()])\n",
        "chistes_n_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHBMq0LsVaXF"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_vocabulary_size = len(spanish_diagnostics_words)\n",
        "spanish_diagnostics_vocabulary_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMJqo8XhVaXF"
      },
      "outputs": [],
      "source": [
        "chistes_vocabulary_size = len(chistes_words)\n",
        "chistes_vocabulary_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLx6HgZOVaXF"
      },
      "source": [
        "La diversidad léxica es una relación entre el tamaño del vocabulario y la cantidad de tokens. Una diversidad léxica mayor denota una riqueza léxica.\n",
        "\n",
        "El corpus spanish_diagnostics presenta una diversidad léxica mayor que el corpus muchocine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCl1csKgVaXF"
      },
      "outputs": [],
      "source": [
        "spanish_diagnostics_lexical_diversity = spanish_diagnostics_vocabulary_size / \\\n",
        "    spanish_diagnostics_n_tokens\n",
        "spanish_diagnostics_lexical_diversity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByKzznHYVaXG"
      },
      "outputs": [],
      "source": [
        "chistes_lexical_diversity = chistes_vocabulary_size / chistes_n_tokens\n",
        "chistes_lexical_diversity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL4W8wBGVaXG"
      },
      "source": [
        "Exploraremos las palabras ordenadas por frecuencia de aparición. Se sabe que los lenguajes naturales obedecen a la ley de Zipf, la cual define que la frecuencia de cualquier palabra es inversamente proporcional a su posición en una tabla de frecuencias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Voj741CkVaXG"
      },
      "source": [
        "Observamos que a medida que vamos avanzando en la posición de frecuencias de las palabras, la frecuencia de cada una cae muy rapidamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or2k15VnVaXG"
      },
      "outputs": [],
      "source": [
        "# Con método collections.Counter.most_common() podemos extraer las palabras más frecuentes del corpus\n",
        "spanish_diagnostics_words_top_words = spanish_diagnostics_words.most_common(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYsboqbGVaXH"
      },
      "outputs": [],
      "source": [
        "chistes_words_top_words = chistes_words.most_common(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0sGkt7TVaXH"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(ncols=2, figsize=(10, 8))\n",
        "axs[0].barh(\n",
        "    [word for word, f in spanish_diagnostics_words_top_words],\n",
        "    [f for word, f in spanish_diagnostics_words_top_words],\n",
        ")\n",
        "axs[1].barh(\n",
        "    [word for word, f in chistes_words_top_words],\n",
        "    [f for word, f in chistes_words_top_words]\n",
        ")\n",
        "axs[0].invert_yaxis()\n",
        "axs[1].invert_yaxis()\n",
        "axs[0].set_title(\"spanish_diagnostics\")\n",
        "axs[1].set_title(\"chistes\")\n",
        "axs[0].set_ylabel(\"Word\")\n",
        "axs[0].set_xlabel(\"Frequency\")\n",
        "axs[1].set_xlabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2l4zNnBVaXH"
      },
      "source": [
        "Para poder visualizar de mejor manera esta característica podemos visualizar las frecuencias con un gráfico de línea sobre unos ejes en escala logarítmica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S54xpo53VaXH"
      },
      "outputs": [],
      "source": [
        "plt.plot([count for word, count in spanish_diagnostics_words.most_common()],\n",
        "         label=\"spanish_diagnostics\")\n",
        "plt.plot([count for word, count in chistes_words.most_common()], label=\"chistes\")\n",
        "plt.yscale(\"log\", base=2)\n",
        "plt.xscale(\"log\", base=2)\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlabel(\"Rank\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmHcoZLnVaXQ"
      },
      "source": [
        "Una de las visualizaciones de corpora más utilizadas es el Word CLoud, la cual es una representación de la frecuencia (u otra métrica asociada) de las palabras mediante el tamaño de las mismas sobre un lienzo.\n",
        "\n",
        "Instanciamos un objeto de la clase wordcloud.WordCloud con los parámetros de la visualización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LRP91Z7VaXQ"
      },
      "outputs": [],
      "source": [
        "wc = wordcloud.WordCloud(  # Objeto de WordCloud\n",
        "    background_color=\"white\",  # El fondo de la visualización es blanco\n",
        "    colormap=\"Blues\"  # Las palabras tendrán color azul\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_05zYST0VaXQ"
      },
      "source": [
        "Nos podemos dar cuenta que hay palabras que tienen un tamaño muy grande (o frecuencia muy alta) pero que no nos ayudan a entender el contenido de los corpora. A estas palabras se les denomina stopwords y típicamente se las elimina para poder realizar nuestro análisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeFNDCZxVaXQ"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, figsize=(10, 10))\n",
        "# Creamos el wordcloud desde las frecuencias de las palabras\n",
        "axs[0].imshow(wc.generate_from_frequencies(\n",
        "    spanish_diagnostics_words), interpolation=\"bilinear\")\n",
        "axs[1].imshow(wc.generate_from_frequencies(\n",
        "    chistes_words), interpolation=\"bilinear\")\n",
        "axs[0].axis(\"off\")\n",
        "axs[1].axis(\"off\")\n",
        "axs[0].set_title(\"spanish_diagnostics\")\n",
        "axs[1].set_title(\"chistes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK8lDNhIVaXR"
      },
      "source": [
        "Existen listas de stopwords típicas en algunas bibliotecas de procesamiento de lenguaje natural."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bGgK-eXVaXR"
      },
      "outputs": [],
      "source": [
        "# La biblioteca Spacy tiene una lista de stopwords en español\n",
        "stopwords = spacy.lang.es.stop_words.STOP_WORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGBgTm8dVaXS"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, figsize=(10, 10))\n",
        "axs[0].imshow(wc.generate_from_frequencies(\n",
        "    {word: f for word, f in spanish_diagnostics_words.items(\n",
        "    ) if not word in stopwords}  # Acá eliminamos las stopwords\n",
        "), interpolation=\"bilinear\")\n",
        "axs[1].imshow(wc.generate_from_frequencies(\n",
        "    {word: f for word, f in chistes_words.items() if not word in stopwords}\n",
        "), interpolation=\"bilinear\")\n",
        "axs[0].axis(\"off\")\n",
        "axs[1].axis(\"off\")\n",
        "axs[0].set_title(\"spanish_diagnostics\")\n",
        "axs[1].set_title(\"chistes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiJNZ_JXVaXS"
      },
      "source": [
        "Si bien una lista de stopwords es normalemnte suficiente para limpiar el texto también podemos extender nuestra lista con palabras que nosotros consideremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTxqd9gQVaXT"
      },
      "outputs": [],
      "source": [
        "stopwords_extended = list(stopwords) + [  # Agregamos estas palabras que nos están ensuciando la visualización\n",
        "    \"y\",\n",
        "    \"a\",\n",
        "    \"o\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNUDogbOVaXT"
      },
      "source": [
        "Ahora con nuestros corpora limpios podemos visualizar fácilmente el contenido de nuestros corpora. En el corpus spanish_diagnostics claramente vemos que las palabras más importantes en la visualización son las relacionadas a salud y en el corpus muchocine son palabras relacionadas al cine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBJ7d8ZQVaXT"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, figsize=(10, 10))\n",
        "axs[0].imshow(wc.generate_from_frequencies(\n",
        "    {word: f for word, f in spanish_diagnostics_words.items()\n",
        "     if not word in stopwords_extended}\n",
        "), interpolation=\"bilinear\")\n",
        "axs[1].imshow(wc.generate_from_frequencies(\n",
        "    {word: f for word, f in chistes_words.items() if not word in stopwords_extended}\n",
        "), interpolation=\"bilinear\")\n",
        "axs[0].axis(\"off\")\n",
        "axs[1].axis(\"off\")\n",
        "axs[0].set_title(\"spanish_diagnostics\")\n",
        "axs[1].set_title(\"chistes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYxnFRG7VaXU"
      },
      "source": [
        "Para poder enriquecer nuestra visualización podemos agregar máscaras a nuestras nubes y así comunicar mejor la información."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9p5LClrVaXU"
      },
      "outputs": [],
      "source": [
        "# Importamos imágenes que utilizaremos como máscaras\n",
        "# Este acceso funciona cargando las imágenes directamente desde la sección actual en Colab,\n",
        "# aunque también se pueden cargar desde Drive.\n",
        "# O bien, si las imágenes se encuentran en la misma carpeta (de forma local)\n",
        "import requests # Importamos la librería requests para descargar imágenes desde URLs\n",
        "from io import BytesIO # Importamos BytesIO para manejar los datos de la imagen en memoria\n",
        "\n",
        "red_cross_link = \"https://raw.githubusercontent.com/dccuchile/CC66Q/main/tutorials/data/red_cross.png\"\n",
        "play_pause_link = \"https://raw.githubusercontent.com/dccuchile/CC66Q/main/tutorials/data/play_pause.png\"\n",
        "\n",
        "# Descargamos las imágenes y las abrimos con PIL\n",
        "response_red_cross = requests.get(red_cross_link)\n",
        "red_cross_image = PIL.Image.open(BytesIO(response_red_cross.content))\n",
        "\n",
        "response_play_pause = requests.get(play_pause_link)\n",
        "play_pause_image = PIL.Image.open(BytesIO(response_play_pause.content))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b93dcee"
      },
      "source": [
        "fig, axs = plt.subplots(nrows=2, figsize=(10, 5))\n",
        "\n",
        "axs[0].imshow(red_cross_image)\n",
        "axs[0].axis(\"off\")\n",
        "\n",
        "axs[1].imshow(play_pause_image)\n",
        "axs[1].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tHG_fbGVaXU"
      },
      "outputs": [],
      "source": [
        "# Convertimos las imágenes a máscaras en formato de array\n",
        "red_cross = np.array(red_cross_image)\n",
        "play_pause = np.array(play_pause_image)\n",
        "\n",
        "# Instanciamos un objeto por cada uno de nuestros corpora\n",
        "wc_spanish_diagnostics = wordcloud.WordCloud(\n",
        "    background_color=\"white\",\n",
        "    colormap=\"Reds\",\n",
        "    mask=red_cross,\n",
        "    contour_width=3,\n",
        "    contour_color='red'\n",
        ")\n",
        "wc_chistes = wordcloud.WordCloud(\n",
        "    background_color=\"white\",\n",
        "    colormap=\"Blues\",\n",
        "    mask=play_pause,\n",
        "    contour_width=3,\n",
        "    contour_color='blue'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXeYCEgrVaXV"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(nrows=2, figsize=(10, 10))\n",
        "axs[0].imshow(wc_spanish_diagnostics.generate_from_frequencies(\n",
        "    {word: f for word, f in spanish_diagnostics_words.items()\n",
        "     if not word in stopwords_extended}\n",
        "), interpolation=\"bilinear\")\n",
        "axs[1].imshow(wc_chistes.generate_from_frequencies(\n",
        "    {word: f for word, f in chistes_words.items() if not word in stopwords_extended}\n",
        "), interpolation=\"bilinear\")\n",
        "axs[0].axis(\"off\")\n",
        "axs[1].axis(\"off\")\n",
        "axs[0].set_title(\"spanish_diagnostics\")\n",
        "axs[1].set_title(\"chistes\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}